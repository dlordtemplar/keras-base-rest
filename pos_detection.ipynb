{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Loading model: model_visualization_siamesedeeplstm\n",
      "Threads:  1790\nQuestions:  1790\nComments:  17900\n",
      "Finished loading.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import model_from_json\n",
    "from loading_preprocessing_TC import *\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "MODEL_DIR = 'out/data/semeval/models'\n",
    "DATASET_PATH = 'resources/datasets/semeval/train/'\n",
    "DATA_PATH = 'out/data/semeval/'\n",
    "MODEL_PATH = 'out/data/semeval/models/'\n",
    "NEURON_COUNT_PATH = 'out/data/semeval/neuron_count.json'\n",
    "POS_PER_NEURON_PATH = 'out/data/semeval/pos_per_neuron.json'\n",
    "\n",
    "MAX_LENGTH = 200\n",
    "model = None\n",
    "tokenizer = None\n",
    "embeddings = None\n",
    "vocabulary_encoded = None\n",
    "vocabulary_inv = None\n",
    "qa_pairs = None\n",
    "answer_texts = None\n",
    "graph = None\n",
    "\n",
    "NEURON_MAX = 128\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load SemEval 2017 files from .xml and convert them into pandas dataframes.\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        train (pandas dataframe): QA-pairs in a format question - correct answers (ids) - pool (ids; incorrect answers).\n",
    "        If there are multiple correct answers to a single question, they are split into multiple QA - pairs.\n",
    "        answer_texts_train (pandas dataframe): answer texts and their ids.\n",
    "    \"\"\"\n",
    "    files = [DATASET_PATH + 'SemEval2016-Task3-CQA-QL-train-part1-subtaskA.xml',\n",
    "             DATASET_PATH + 'SemEval2016-Task3-CQA-QL-train-part2-subtaskA.xml']\n",
    "    train_xml = read_xml(files)\n",
    "    train, answer_texts_train = xml2dataframe_Labels(train_xml, 'train')\n",
    "    answer_texts_train.set_index('answer_id', drop=False, inplace=True)\n",
    "    return train, answer_texts_train\n",
    "\n",
    "\n",
    "def load_model(new_model_filename):\n",
    "    \"\"\"Load a pretrained model from PyTorch / Keras checkpoint.\n",
    "    Args:\n",
    "        new_model_filename (string): the name of the model used when saving its weights and architecture to\n",
    "        either a binary (PyTorch) or a .h5 and a .json (Keras)\n",
    "\n",
    "    Returns:\n",
    "        error (string): The error message displayed to a user. If empty, counts as no error.\n",
    "    \"\"\"\n",
    "    global model, model_filename\n",
    "    print(\"Loading model:\", new_model_filename)\n",
    "    try:\n",
    "        json_file = open(MODEL_PATH + new_model_filename + '.json',\n",
    "                         'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        global graph\n",
    "        graph = tf.get_default_graph()\n",
    "        # load weights into new model\n",
    "        model.load_weights(MODEL_PATH + new_model_filename + \".h5\")\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model_filename = new_model_filename\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        error = \"<div class=\\\"alert alert-warning\\\"> Sorry, there is something wrong with the model: <br> \" + str(\n",
    "            e) + \"</div>\"\n",
    "        return error\n",
    "    \n",
    "\n",
    "def load_environment():\n",
    "    \"\"\"Load documents index for search engine, pre-trained embeddings, vocabulary, parameters and the model.\"\"\"\n",
    "    global model, tokenizer, embeddings, vocabulary_encoded, vocabulary_inv, qa_pairs, answer_texts, graph\n",
    "    with open(DATA_PATH + 'tokenizer.p', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    with open(DATA_PATH + 'embedding_matrix.p', 'rb') as handle:\n",
    "        embeddings = pickle.load(handle)\n",
    "    vocabulary_encoded = tokenizer.word_index\n",
    "    vocabulary_inv = {v: k for k, v in vocabulary_encoded.items()}\n",
    "    model = load_model('model_visualization_siamesedeeplstm')\n",
    "    qa_pairs, answer_texts = load_data()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_data(texts):\n",
    "    \"\"\"Tokenize texts and pad resulting sequences of words using Keras functions.\"\"\"\n",
    "    global tokenizer, embeddings\n",
    "    tokens = tokenizer.texts_to_sequences(texts)\n",
    "    padded_tokens = pad_sequences(tokens, maxlen=MAX_LENGTH, value=embeddings.shape[0] - 1)\n",
    "    return tokens, padded_tokens\n",
    "\n",
    "\n",
    "def visualize_model_deep(model, question_lstm=True):\n",
    "    \"\"\"Retrieve weights of the second shared LSTM to visualize neuron activations.\"\"\"\n",
    "    recurrent_layer = model.get_layer('SharedLSTM2')\n",
    "    output_layer = model.layers[-1]\n",
    "\n",
    "    inputs = []\n",
    "    inputs.extend(model.inputs)\n",
    "\n",
    "    outputs = []\n",
    "    outputs.extend(model.outputs)\n",
    "    if question_lstm:\n",
    "        outputs.append(recurrent_layer.get_output_at(1))\n",
    "    else:\n",
    "        outputs.append(recurrent_layer.get_output_at(0))\n",
    "\n",
    "    global graph\n",
    "    with graph.as_default():\n",
    "        all_function = K.function(inputs, outputs)\n",
    "        output_function = K.function([output_layer.input], model.outputs)\n",
    "    return all_function, output_function\n",
    "\n",
    "def highlight_neuron(rnn_values, texts, tokens, scale, neuron):\n",
    "    \"\"\"Generate HTML code where each word is highlighted according to a given neuron activity on it.\"\"\"\n",
    "    tag_string = \"<span data-toggle=\\\"tooltip\\\" title=\\\"SCORE\\\"><span style = \\\"background-color: rgba(COLOR, OPACITY);\\\">WORD</span></span>\"\n",
    "    old_texts = texts\n",
    "    texts = []\n",
    "    for idx in range(0, len(old_texts)):\n",
    "        current_neuron_values = rnn_values[idx, :, neuron]\n",
    "        current_neuron_values = current_neuron_values[-len(tokens[idx]):]\n",
    "        words = [vocabulary_inv[x] for x in tokens[idx]]\n",
    "        current_strings = []\n",
    "        if scale:\n",
    "            scaled = [\n",
    "                ((x - min(current_neuron_values)) * (2) / (\n",
    "                        max(current_neuron_values) - min(current_neuron_values))) + (\n",
    "                    -1)\n",
    "                for x in current_neuron_values]\n",
    "        else:\n",
    "            scaled = current_neuron_values\n",
    "        for score, word, scaled_score in zip(current_neuron_values, words, scaled):\n",
    "            if score > 0:\n",
    "                color = '195, 85, 58'\n",
    "            else:\n",
    "                color = '63, 127, 147'\n",
    "            current_string = tag_string.replace('SCORE', str(score)).replace('WORD', word).replace('OPACITY', str(\n",
    "                abs(scaled_score))).replace('COLOR', color)\n",
    "            current_strings.append(current_string)\n",
    "        texts.append(' '.join(current_strings))\n",
    "    return texts\n",
    "\n",
    "\n",
    "model = load_environment()\n",
    "print(\"Finished loading.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1790\nLoading existing file.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Generate pos_tagged_questions \n",
    "\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "POS_TAGGED_QUESTIONS_PATH = 'out/data/semeval/pos_tagged_questions.json'\n",
    "\n",
    "\n",
    "print(len(qa_pairs))\n",
    "if not os.path.isfile(POS_TAGGED_QUESTIONS_PATH):\n",
    "    print('File not found! Creating new version...')\n",
    "    tagged_dict = {}\n",
    "    for i in range(len(qa_pairs)):\n",
    "        row = qa_pairs.iloc[i]\n",
    "        question = row['question']\n",
    "        \n",
    "        tokens = nltk.word_tokenize(question)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        tagged_dict[i] = tagged\n",
    "    print('Writing to file...')\n",
    "    with open(POS_TAGGED_QUESTIONS_PATH, 'w') as file:\n",
    "        json.dump(tagged_dict, file)\n",
    "        print('Finished!')\n",
    "else:\n",
    "    print('Loading existing file.')\n",
    "    with open(POS_TAGGED_QUESTIONS_PATH, 'r') as file:\n",
    "        tagged_dict = json.load(file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "indices = [0, 1]\n",
    "neuron = 0\n",
    "\n",
    "# Start actual visualization\n",
    "all_highlighted_wrong_answers = []\n",
    "all_wrong_answers = []\n",
    "\n",
    "min_ca = 1\n",
    "\n",
    "min_wa = 1\n",
    "max_ca = -1\n",
    "max_wa = -1\n",
    "\n",
    "activated_words = []\n",
    "activated_words_values = []\n",
    "antiactivated_words = []\n",
    "antiactivated_words_values = []\n",
    "\n",
    "activation_per_word_data = {}\n",
    "asked_questions = {}\n",
    "\n",
    "all_function_deep, output_function_deep = visualize_model_deep(model, False)\n",
    "nlp = spacy.load('en')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loading existing file...\nDone.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "neuron_counts\n",
    "\n",
    "[\n",
    "    {\n",
    "        num: 0,\n",
    "        # tokens = [('Yes', 'UH'), ('Try', 'VB'), ...]\n",
    "        token_counts: {\n",
    "            UH: 1,\n",
    "            VB: 1,\n",
    "            ...\n",
    "        }\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\"\"\"\n",
    "neuron_counts = []\n",
    "\n",
    "if os.path.exists(NEURON_COUNT_PATH):\n",
    "    print('Loading existing file...')\n",
    "    with open(NEURON_COUNT_PATH, 'r') as file:\n",
    "        neuron_counts = json.load(file)\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('Generating new file...')\n",
    "    pass\n",
    "\n",
    "# for i in range(87, 89): #indices:\n",
    "#     print('Generating activations for QA pair', i)\n",
    "#     neuron_count = {\n",
    "#         'num': i,\n",
    "#         'tokens': [],\n",
    "#         'token_counts': {}\n",
    "#     }\n",
    "#     row = qa_pairs.iloc[i]\n",
    "#     correct_answers = answer_texts.loc[row['answer_ids']]['answer'].values\n",
    "#     # correct_answers_pos = []\n",
    "#     # for sent in correct_answers:\n",
    "#     #     # tokens = nltk.word_tokenize(sent)\n",
    "#     #     tokens = nlp(sent)\n",
    "#     #     nlp_tokens = [str(token.text) for token in tokens]\n",
    "#     #     correct_answers_pos.append(nltk.pos_tag(nlp_tokens))\n",
    "#     wrong_answers = answer_texts.loc[row['pool']]['answer'].values\n",
    "#     question = row['question']\n",
    "#     asked_questions[i] = question\n",
    "#     q_tokens, q_padded_tokens = prepare_data([question])\n",
    "#     ca_tokens, ca_padded_tokens = prepare_data(correct_answers)\n",
    "#     wa_tokens, wa_padded_tokens = prepare_data(wrong_answers)\n",
    "#     if len(correct_answers) > 0:\n",
    "#         scores_ca, rnn_values_ca = all_function_deep([q_padded_tokens * len(correct_answers), ca_padded_tokens])\n",
    "#         # neuron_num = rnn_values_ca.shape[-1]\n",
    "#         all_values_ca = rnn_values_ca[:, :, neuron:neuron + 1]\n",
    "#         # if np.min(all_values_ca) < min_ca:\n",
    "#         #     min_ca = np.min(all_values_ca)\n",
    "#         # if np.max(all_values_ca) > max_ca:\n",
    "#         #     max_ca = np.max(all_values_ca)\n",
    "#             \n",
    "#         for idx in range(0, len(correct_answers)):\n",
    "#             current_neuron_values = rnn_values_ca[idx, :, neuron]\n",
    "#             current_neuron_values = current_neuron_values[-len(ca_tokens[idx]):]\n",
    "#             # print(correct_answers[idx]) # Yes. It is right behind Kahrama in the National area.\n",
    "#             # print(ca_tokens[idx]) # [163, 11, 7, 139, 651, 12356, 6, 1, 1083, 363]\n",
    "#             # print(current_neuron_values) # [5.8287954e-01 2.0389782e-02 3.1098869e-04 3.2598805e-04 1.8663764e-04 1.3387189e-03 1.0951190e-04 6.6894456e-04 5.4823589e-03 5.3216936e-03]\n",
    "#             \n",
    "#             tokens = keras.preprocessing.text.text_to_word_sequence(correct_answers[idx], lower=True)\n",
    "#             tagged = nltk.pos_tag(tokens)\n",
    "#             \n",
    "#             if len(tagged) > 0:\n",
    "#                     \n",
    "#                 val_max = max(current_neuron_values)\n",
    "#                 val_min = min(current_neuron_values)\n",
    "#                 index_min = np.argmin(current_neuron_values)\n",
    "#                 index_max = np.argmax(current_neuron_values)\n",
    "#                 # print('max', val_max)\n",
    "#                 # print('min', val_min)\n",
    "#                 index_chosen = -1\n",
    "#                 if abs(val_min) > val_max:\n",
    "#                     index_chosen = index_min\n",
    "#                 else:\n",
    "#                     index_chosen = index_max\n",
    "#                 # print(index_chosen)\n",
    "#                 # print(keras.preprocessing.text.text_to_word_sequence(correct_answers[idx], lower=True))\n",
    "#                 \n",
    "#                 # print(tagged)\n",
    "#                 token_to_add = tagged[index_chosen]\n",
    "#                 # print(token_to_add)\n",
    "#                 # neuron_count['tokens'].append(token_to_add)\n",
    "#                 if token_to_add[1] in neuron_count['token_counts']:\n",
    "#                     neuron_count['token_counts'][token_to_add[1]] = neuron_count['token_counts'][token_to_add[1]] + 1\n",
    "#                 else:\n",
    "#                     neuron_count['token_counts'][token_to_add[1]] = 1\n",
    "#             else:\n",
    "#                 print('No tagged tokens!')\n",
    "#         \n",
    "#             \n",
    "#             \n",
    "#         # highlighted_correct_answers = highlight_neuron(rnn_values_ca, correct_answers, ca_tokens,\n",
    "#         #                                                False,  # Scale placeholder\n",
    "#         #                                                neuron)\n",
    "#         \n",
    "#         \n",
    "#         \n",
    "#         # print(tagged_dict[str(i)])\n",
    "#         \n",
    "#         # if i not in indexed_highlighted_correct_answers:\n",
    "#         #     indexed_highlighted_correct_answers[i] = [highlighted_correct_answers]\n",
    "#         # else:\n",
    "#         #     indexed_highlighted_correct_answers[i].append(highlighted_correct_answers)\n",
    "#         # \n",
    "#         # current_ca = [[vocabulary_inv[x] for x in ca_tokens[idx]] for idx in range(len(ca_tokens))]\n",
    "#         # if i not in indexed_correct_answers:\n",
    "#         #     indexed_correct_answers[i] = current_ca\n",
    "#         # else:\n",
    "#         #     indexed_correct_answers[i].append(current_ca)\n",
    "# \n",
    "#         # activation_per_word_data['ca_firings' + str(i)] = rnn_values_ca[:, :, neuron].flatten()\n",
    "#         # activation_per_word_data['ca_text' + str(i)] = [\n",
    "#         #     vocabulary_inv[token] if token in vocabulary_inv.keys() else '<pad>' for x in ca_padded_tokens for\n",
    "#         #     token\n",
    "#         #     in x]\n",
    "#     else:\n",
    "#         pass\n",
    "#         # if i not in indexed_highlighted_correct_answers:\n",
    "#         #     indexed_highlighted_correct_answers[i] = []\n",
    "#         # else:\n",
    "#         #     indexed_highlighted_correct_answers[i].append([])\n",
    "#         # \n",
    "#         # if i not in indexed_correct_answers:\n",
    "#         #     indexed_correct_answers[i] = []\n",
    "#         # else:\n",
    "#         #     indexed_correct_answers[i].append([])\n",
    "# \n",
    "#         # activation_per_word_data['ca_text' + str(i)] = []\n",
    "#         # activation_per_word_data['ca_firings' + str(i)] = []\n",
    "# \n",
    "#     if len(wrong_answers) > 0:\n",
    "#         scores_wa, rnn_values_wa = all_function_deep([q_padded_tokens * len(wrong_answers), wa_padded_tokens])\n",
    "#         # neuron_num = rnn_values_wa.shape[-1]\n",
    "#         # all_values_wa = rnn_values_wa[:, :, neuron:neuron + 1]\n",
    "#         # if np.min(all_values_wa) < min_wa:\n",
    "#         #     min_wa = np.min(all_values_wa)\n",
    "#         # if np.max(all_values_wa) > max_wa:\n",
    "#         #     max_wa = np.max(all_values_wa)\n",
    "#             \n",
    "#         for idx in range(0, len(wrong_answers)):\n",
    "#             current_neuron_values = rnn_values_wa[idx, :, neuron]\n",
    "#             current_neuron_values = current_neuron_values[-len(wa_tokens[idx]):]\n",
    "#             # print('wa_tokens[idx]', wa_tokens[idx])\n",
    "#             # print(correct_answers[idx]) # Yes. It is right behind Kahrama in the National area.\n",
    "#             # print(ca_tokens[idx]) # [163, 11, 7, 139, 651, 12356, 6, 1, 1083, 363]\n",
    "#             # print(current_neuron_values) # [5.8287954e-01 2.0389782e-02 3.1098869e-04 3.2598805e-04 1.8663764e-04 1.3387189e-03 1.0951190e-04 6.6894456e-04 5.4823589e-03 5.3216936e-03]\n",
    "#             \n",
    "#             tokens = keras.preprocessing.text.text_to_word_sequence(wrong_answers[idx], lower=True)\n",
    "#             tagged = nltk.pos_tag(tokens)\n",
    "#             \n",
    "#             if len(tagged) > 0:\n",
    "#                 val_max = max(current_neuron_values)\n",
    "#                 val_min = min(current_neuron_values)\n",
    "#                 index_min = np.argmin(current_neuron_values)\n",
    "#                 index_max = np.argmax(current_neuron_values)\n",
    "#                 # print('max', val_max)\n",
    "#                 # print('min', val_min)\n",
    "#                 index_chosen = -1\n",
    "#             \n",
    "#                 if abs(val_min) > val_max:\n",
    "#                     index_chosen = index_min\n",
    "#                 elif abs(val_min) < val_max: \n",
    "#                     index_chosen = index_max\n",
    "#                 else:\n",
    "#                     pass\n",
    "#                 # print(index_chosen)\n",
    "#                 \n",
    "#                 # print('wa')\n",
    "#                 # print(wrong_answers)\n",
    "#                 # print(wrong_answers[idx])\n",
    "#                 \n",
    "#                 # print(tagged)\n",
    "#                 # print(index_chosen)\n",
    "#                 token_to_add = tagged[index_chosen]\n",
    "#                 # print(token_to_add)\n",
    "#                 neuron_count['tokens'].append(token_to_add)\n",
    "#                 if token_to_add[1] in neuron_count['token_counts']:\n",
    "#                     neuron_count['token_counts'][token_to_add[1]] = neuron_count['token_counts'][token_to_add[1]] + 1\n",
    "#                 else:\n",
    "#                     neuron_count['token_counts'][token_to_add[1]] = 1\n",
    "#             else:\n",
    "#                 print('No tagged tokens!')\n",
    "#             \n",
    "#         # highlighted_wrong_answers = highlight_neuron(rnn_values_wa, wrong_answers, wa_tokens, False,\n",
    "#         #                                              # Scale placeholder\n",
    "#         #                                              neuron)\n",
    "#         # all_highlighted_wrong_answers.append(highlighted_wrong_answers)\n",
    "# \n",
    "#         # if i not in indexed_highlighted_wrong_answers:\n",
    "#         #     indexed_highlighted_wrong_answers[i] = [highlighted_wrong_answers]\n",
    "#         # else:\n",
    "#         #     indexed_highlighted_wrong_answers[i].append(highlighted_wrong_answers)\n",
    "#         # \n",
    "#         # current_wa = [[vocabulary_inv[x] for x in wa_tokens[idx]] for idx in range(len(wa_tokens))]\n",
    "#         # if i not in indexed_wrong_answers:\n",
    "#         #     indexed_wrong_answers[i] = current_wa\n",
    "#         # else:\n",
    "#         #     indexed_wrong_answers[i].append(current_wa)\n",
    "# \n",
    "#         # activation_per_word_data['wa_firings' + str(i)] = rnn_values_wa[:, :, neuron].flatten()\n",
    "#         # activation_per_word_data['wa_text' + str(i)] = [\n",
    "#         #     vocabulary_inv[token] if token in vocabulary_inv.keys() else '<pad>' for x in wa_padded_tokens for\n",
    "#         #     token\n",
    "#         #     in x]\n",
    "#     else:\n",
    "#         pass\n",
    "#         # all_highlighted_wrong_answers.append([])\n",
    "# \n",
    "#         # if i not in indexed_highlighted_wrong_answers:\n",
    "#         #     indexed_highlighted_wrong_answers[i] = []\n",
    "#         # else:\n",
    "#         #     indexed_highlighted_wrong_answers[i].append([])\n",
    "#         # \n",
    "#         # all_wrong_answers.append([])\n",
    "#         # \n",
    "#         # if i not in indexed_wrong_answers:\n",
    "#         #     indexed_wrong_answers[i] = []\n",
    "#         # else:\n",
    "#         #     indexed_wrong_answers[i].append([])\n",
    "# \n",
    "#         # activation_per_word_data['wa_text' + str(i)] = []\n",
    "#         # activation_per_word_data['wa_firings' + str(i)] = []\n",
    "#         \n",
    "#     # print(neuron_count)\n",
    "#     neuron_counts.append(neuron_count)\n",
    "    \n",
    "# with open(NEURON_COUNT_PATH, 'w') as file:\n",
    "#         json.dump(neuron_counts, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['$', \"''\", 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "key_set = set()\n",
    "for item in neuron_counts:\n",
    "    for key in list(item['token_counts'].keys()):\n",
    "        key_set.add(key)\n",
    "\n",
    "# ['$', \"''\", 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "print(sorted(key_set))\n",
    "key_list = ['$', \"''\", 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "\n",
    "neuron_nums = []\n",
    "for num in range(128):\n",
    "    neuron_nums.append('Neuron ' + str(num))\n",
    "# print(neuron_nums)\n",
    "\n",
    "key_per_neuron = {'$': [], \"''\": [], 'CC': [], 'CD': [], 'DT': [], 'EX': [],\n",
    "                  'FW': [], 'IN': [], 'JJ': [], 'JJR': [], 'JJS': [], 'MD': [],\n",
    "                  'NN': [], 'NNP': [], 'NNPS': [], 'NNS': [], 'PDT': [], 'POS': [],\n",
    "                  'PRP': [], 'PRP$': [], 'RB': [], 'RBR': [], 'RBS': [], 'RP': [],\n",
    "                  'TO': [], 'UH': [], 'VB': [], 'VBD': [], 'VBG': [], 'VBN': [],\n",
    "                  'VBP': [], 'VBZ': [], 'WDT': [], 'WP': [], 'WP$': [], 'WRB': []\n",
    "                  }\n",
    "for neuron in neuron_counts:\n",
    "    for key in key_list:\n",
    "        if key not in neuron['token_counts']:\n",
    "            key_per_neuron[key].append(0)\n",
    "        else:\n",
    "            key_per_neuron[key].append(neuron['token_counts'][key])\n",
    "# print(key_per_neuron)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Values\n",
    "trace_x = []\n",
    "# Labels\n",
    "trace_y = []\n",
    "\n",
    "plotly_tsne = []\n",
    "for key in key_list:\n",
    "    trace_neuron = {\n",
    "        'name': key,\n",
    "        'x': key_per_neuron[key],\n",
    "        'y': neuron_nums,\n",
    "        'orientation': 'h',\n",
    "        'type': 'bar'\n",
    "    }\n",
    "    plotly_tsne.append(trace_neuron)\n",
    "\n",
    "plotly_tsne_as_json = pd.Series(plotly_tsne).to_json(orient='values')\n",
    "with open(POS_PER_NEURON_PATH, 'w') as file:\n",
    "    json.dump(plotly_tsne_as_json, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}